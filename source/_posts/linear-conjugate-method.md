---
title: 线性共轭梯度法
date: 2019-11-07 00:01:26
tags:
mathjax: true
---

The conjugate gradient method is an iterative method for solving a minimization problem
$$
\phi(x)=\frac{1}{2} x^{T} A x-b^{T} x,
$$
where $A$ is an $n \times n$ matrix that is symmetric and positive definite. The gradient of $\phi$ equals the residual of the linear system
$$
\nabla \phi(x)=A x-b \stackrel{\mathrm{def}}{=} r(x).
$$

<!--more-->

## Conjugacy

A set of nonzero vectors $\{p_0, p_1, ..., p_l\}$ is said to be **conjugate** with respect to the symmetric positive definite matrix $A$ if

$$
p_{i}^{T} A p_{j}=0, \quad \text { for all } i \neq j.
$$

Any set of vectors satisfying this property is also linearly independent.

## Conjugate Direction Method

Given a starting point $x_0 \in \mathbb{R}^n$ and a set of conjugate directions $\{p_0, p_1, ..., p_{n-1}\}$, the method generate the sequence ${x_k}$ by setting
$$
x_{k+1}=x_{k}+\alpha_{k} p_{k},
$$
where $\alpha_k$ is the one-dimensional minimizer of the quadratic function $\phi(·)$ along $x_k + \alpha p_k$, given explicitly by
$$
\alpha_{k}=-\frac{r_{k}^{T} p_{k}}{p_{k}^{T} A p_{k}}.
$$

__Convergence of Algorithm__

For any $x_0 \in \mathbb{R}^n$ the sequence $\{x_k\}$ generated by the conjugate direction algorithm converges to the solution $x^*$ in at most $n$ steps.

__Expanding Subspace Minimization__

Let $x_0 \in \mathbb{R}^n$ be any starting point and suppose that the sequence $\{x_k\}$ is generated by the conjugate direction algorithm. Then
$$
r_{k}^{T} p_{i}=0, \quad \text { for } i=0, \ldots, k-1,
$$
and $x_k$ is the minimizer of $\phi(x)=\frac{1}{2} x^{T} A x-b^{T} x$ over the set
$$
\left\{x | x=x_{0}+\operatorname{span}\left\{p_{0}, p_{1}, \ldots, p_{k-1}\right\}\right\}.
$$

## Conjugate Gradient Method  (Preliminary Version)

The **conjugate gradient method** is a conjugate direction method with a very special property: In generating its set of conjugate vectors, it can compute a new vector $p_k$ by using only the previous vector $p_{k−1}$,
$$
p_{k}=-r_{k}+\beta_{k} p_{k-1},
$$
where
$$
\beta_{k}=\frac{r_{k}^{T} A p_{k-1}}{p_{k-1}^{T} A p_{k-1}}.
$$
__Algorithm__

> **CG - Preliminary Version**
>
> - Given $x_0$;
> - Set $r_{0} \leftarrow A x_{0}-b, p_{0} \leftarrow-r_{0}, k \leftarrow 0$;
> - **while** $r_{k} \neq 0$
>
> $$
> \begin{aligned} \alpha_{k} & \leftarrow-\frac{r_{k}^{T} p_{k}}{p_{k}^{T} A p_{k}} \\ x_{k+1} & \leftarrow x_{k}+\alpha_{k} p_{k} \\ r_{k+1} & \leftarrow A x_{k+1}-b \\ \beta_{k+1} & \leftarrow \frac{r_{k+1}^{T} A p_{k}}{p_{k}^{T} A p_{k}} \\ p_{k+1} & \leftarrow-r_{k+1}+\beta_{k+1} p_{k} \\ k & \leftarrow k+1 \end{aligned}
> $$
>
> - **end(while)**

__Properties__

+ The directions $\left\{p_{0}, p_{1}, \ldots, p_{n-1}\right\}$ are indeed conjugate. Therefore, the sequence $\{x_k\}$ converges to $x^*$ in at most $n$ steps.

$$
p_{k}^{T} A p_{i}=0, \quad \text { for } i=0,1, \ldots, k-1
$$

+ The residuals $r_i$ are mutually orthogonal.

$$
r_{k}^{T} r_{i}=0, \quad \text { for } i=0, \ldots, k-1
$$

+ Each search direction $p_k$ is contained in the **Krylov subspace** of degree $k$ for $r_0$.

$$
\operatorname{span}\left\{p_{0}, p_{1}, \ldots, p_{k}\right\}=\operatorname{span}\left\{r_{0}, A r_{0}, \ldots, A^{k} r_{0}\right\}
$$

+ Each search direction $r_k$ is contained in the **Krylov subspace** of degree $k$ for $r_0$.

$$
\operatorname{span}\left\{r_{0}, r_{1}, \ldots, r_{k}\right\}=\operatorname{span}\left\{r_{0}, A r_{0}, \ldots, A^{k} r_{0}\right\}
$$

## Conjugate Gradient Method  (Practical Form)

__Algorithm__

> **CG**
>
> - Given $x_0$;
> - Set $r_{0} \leftarrow A x_{0}-b, p_{0} \leftarrow-r_{0}, k \leftarrow 0$;
> - **while** $r_{k} \neq 0$
>
> $$
> \begin{aligned} \alpha_{k} & \leftarrow \frac{r_{k}^{T} r_{k}}{p_{k}^{T} A p_{k}} \\ x_{k+1} & \leftarrow x_{k}+\alpha_{k} p_{k} \\ r_{k+1} & \leftarrow r_{k}+\alpha_{k} A p_{k} \\ \beta_{k+1} & \leftarrow \frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}} \\ p_{k+1} & \leftarrow-r_{k+1}+\beta_{k+1} p_{k} \\ k & \leftarrow k+1 \end{aligned}
> $$
>
> - **end(while)**

## Rate of Convergence

__Analysis__
$$
\begin{aligned} 
x_{k+1} &=x_{0}+\alpha_{0} p_{0}+\cdots+\alpha_{k} p_{k} \\
&=x_{0}+\gamma_{0} r_{0}+\gamma_{1} A r_{0}+\cdots+\gamma_{k} A^{k} r_{0} \\
&= x_{0}+P_{k}^{*}(A) r_{0}
\end{aligned}
$$
The polynomial $P_{k}^{*}$ solves the problem in which the minimum is taken over the space of all possible polynomials of degree $k$:
$$
\min _{P_{k}}\left\|x_{0}+P_{k}(A) r_{0}-x^{*}\right\|_{A},
$$
where the weighted norm measure $\|\cdot\|_{A}$ is defined by $\|z\|_{A}^{2}=z^{T} A z$.

Let $0<\lambda_{1} \leq \lambda_{2} \leq \cdots \leq \lambda_{n}$ be the eigenvalues of $A$, and let $v_{1}, v_{2}, \dots, v_{n}$ be the corresponding orthonormal eigenvectors. Assumed that
$$
x_{0}-x^{*}=\sum_{i=1}^{n} \xi_{i} v_{i},
$$
then
$$
\begin{aligned}
\left\|x_{k+1}-x^{*}\right\|_{A}^{2} &= \sum_{i=1}^{n} \lambda_{i}\left[1+\lambda_{i} P_{k}^{*}\left(\lambda_{i}\right)\right]^{2} \xi_{i}^{2} \\ &= \min _{P_{k}} \sum_{i=1}^{n} \lambda_{i}\left[1+\lambda_{i} P_{k}\left(\lambda_{i}\right)\right]^{2} \xi_{i}^{2}.
\end{aligned}
$$
__Estimation - 1__
$$
\begin{aligned}\left\|x_{k+1}-x^{*}\right\|_{A}^{2} & \leq \min _{P_{k}} \max _{1 \leq i \leq n}\left[1+\lambda_{i} P_{k}\left(\lambda_{i}\right)\right]^{2}\left(\sum_{j=1}^{n} \lambda_{j} \xi_{j}^{2}\right) \\ &=\min _{P_{k}} \max _{1 \leq i \leq n}\left[1+\lambda_{i} P_{k}\left(\lambda_{i}\right)\right]^{2}\left\|x_{0}-x^{*}\right\|_{A}^{2} \end{aligned}
$$
__Conclusion__

If $A$ has only $r$ distinct eigenvalues, then the CG iteration will terminate at the solution in at most $r$ iterations.

__Estimation - 2__
$$
\left\|x_{k+1}-x^{*}\right\|_{A}^{2} \leq\left(\frac{\lambda_{n-k}-\lambda_{1}}{\lambda_{n-k}+\lambda_{1}}\right)^{2}\left\|x_{0}-x^{*}\right\|_{A}^{2}
$$
__Estimation - 3__
$$
\left\|x_{k}-x^{*}\right\|_{A} \leq\left(\frac{\sqrt{\kappa(A)}-1}{\sqrt{\kappa(A)}+1}\right)^{2 k}\left\|x_{0}-x^{*}\right\|_{A}
$$
where $\kappa(A)$ is the Euclidean condition number of $A$:
$$
\kappa(A)=\|A\|_{2}\left\|A^{-1}\right\|_{2}=\lambda_{1} / \lambda_{n}.
$$

## Preconditioning

A change of variables from $x$ to $\hat{x}$ via a nonsingular matrix $C$:
$$
\hat{x}=C x.
$$
The new linear system:
$$
\left(C^{-T} A C^{-1}\right) \hat{x}=C^{-T} b
$$
The algorithm does not make use of $C$ explicitly, but rather the matrix $M = C^TC$.

__Algorithm__

> **Preconditioned CG**
>
> + Give $x_0$, preconditioner $M$;
> + Set $r_{0} \leftarrow A x_{0}-b$;
> + Solve $My_0 = r_0$ for $y_0$;
> + Set $p_{0} = -y_{0}, k \leftarrow 0$;
> + **while** $r_{k} \neq 0$
>
> $$
> \begin{aligned} \alpha_{k} & \leftarrow \frac{r_{k}^{T} y_{k}}{p_{k}^{T} A p_{k}} \\ x_{k+1} & \leftarrow x_{k}+\alpha_{k} p_{k}  \\ r_{k+1} & \leftarrow r_{k}+\alpha_{k} A p_{k} \\ M y_{k+1} & \leftarrow r_{k+1} \\ \beta_{k+1} & \leftarrow \frac{r_{k+1}^{T} y_{k+1}}{r_{k}^{T} y_{k}} \\ p_{k+1} & \leftarrow-y_{k+1}+\beta_{k+1} p_{k} \\ k & \leftarrow k+1 \end{aligned}
> $$
>
> + **end(while)**

__Property__
$$
r_{i}^{T} M^{-1} r_{j}=0 \quad \text { for all } i \neq j
$$
